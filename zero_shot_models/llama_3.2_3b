import os
import sys
import pandas as pd
import numpy as np
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
from tqdm import tqdm
import json
import argparse
from datetime import datetime
import gc
from sklearn.metrics import classification_report, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns
from huggingface_hub import login
from collections import Counter
import logging

# Set up logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

def parse_args():
    """Parse command line arguments."""
    parser = argparse.ArgumentParser(
        description='Zero-shot binary classification for regional bias detection using Mistral')
    
    # Data and output paths
    parser.add_argument('--data_path', type=str, required=True,
                        help='Path to the annotated dataset CSV file')
    parser.add_argument('--output_dir', type=str, default='results/zero_shot/',
                        help='Directory to save output files')
    parser.add_argument('--cache_dir', type=str, default=None,
                        help='Directory for model cache')
    
    # Model configuration
    parser.add_argument('--model_name', type=str, 
                        default='mistralai/Mistral-7B-Instruct-v0.3',
                        help='Model name or path')
    parser.add_argument('--gpu_id', type=int, default=0,
                        help='GPU ID to use for inference')
    parser.add_argument('--hf_token', type=str, default=None,
                        help='HuggingFace token for accessing gated models')
    
    # Execution parameters
    parser.add_argument('--num_iterations', type=int, default=3,
                        help='Number of classification iterations to run')
    parser.add_argument('--batch_size', type=int, default=1,
                        help='Batch size for processing (default: 1)')
    parser.add_argument('--max_samples', type=int, default=None,
                        help='Maximum number of samples to process (for testing)')
    parser.add_argument('--max_length', type=int, default=2048,
                        help='Maximum context length for tokenizer')
    parser.add_argument('--max_new_tokens', type=int, default=300,
                        help='Maximum number of new tokens to generate')
    
    args = parser.parse_args()
    return args

class RegionalBiasClassifier:
    def __init__(self, model_name="mistralai/Mistral-7B-Instruct-v0.3", 
                 gpu_id=0, cache_dir=None, hf_token=None):
        """
        Initialize the classifier with the specified model.
        
        Args:
            model_name: Name of the model to load
            gpu_id: GPU ID to use for computation
            cache_dir: Directory for model cache
            hf_token: HuggingFace token for accessing gated models
        """
        # Set GPU device
        self.gpu_id = gpu_id
        os.environ['CUDA_VISIBLE_DEVICES'] = str(gpu_id)
        torch.cuda.set_device(0)  # After setting CUDA_VISIBLE_DEVICES, we use device 0
        self.device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
        
        logger.info(f"Initializing {model_name}")
        if torch.cuda.is_available():
            logger.info(f"Using GPU: {torch.cuda.get_device_name(0)}")
            logger.info(f"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB")
        else:
            logger.warning("CUDA not available. Using CPU (this will be very slow).")
        
        # Clear GPU cache before loading
        torch.cuda.empty_cache()
        gc.collect()
        
        # Initialize tokenizer with appropriate settings
        tokenizer_kwargs = {
            'padding_side': 'left'  # Better for decoder-only models
        }
        
        if cache_dir:
            tokenizer_kwargs['cache_dir'] = cache_dir
        if hf_token:
            tokenizer_kwargs['token'] = hf_token
            
        self.tokenizer = AutoTokenizer.from_pretrained(model_name, **tokenizer_kwargs)
        
        # Ensure tokenizer has padding token
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
        
        # Set up model loading configuration based on available hardware
        model_kwargs = {
            'torch_dtype': torch.float16,
            'low_cpu_mem_usage': True,
        }
        
        # Add optional parameters if provided
        if cache_dir:
            model_kwargs['cache_dir'] = cache_dir
        if hf_token:
            model_kwargs['token'] = hf_token
            
        # Configure device mapping based on available GPU memory
        if torch.cuda.is_available():
            # Compute available GPU memory in GB
            available_memory = torch.cuda.get_device_properties(0).total_memory / 1e9
            max_memory_str = f"{int(available_memory * 0.85)}GB"  # Use 85% of available memory
            
            model_kwargs['device_map'] = {'': 0}  # Force everything to specified GPU
            model_kwargs['max_memory'] = {0: max_memory_str}  # Limit memory usage
            
            logger.info(f"Setting max GPU memory usage to {max_memory_str}")
        
        # Load the model with optimized settings
        self.model = AutoModelForCausalLM.from_pretrained(model_name, **model_kwargs)
        
        # Set to evaluation mode
        self.model.eval()
        
        # Log memory usage
        if torch.cuda.is_available():
            logger.info(f"Model loaded. Memory allocated: {torch.cuda.memory_allocated(0) / 1e9:.2f} GB")
            logger.info(f"Memory reserved: {torch.cuda.memory_reserved(0) / 1e9:.2f} GB")
        
    def classify_with_cot(self, text, max_length=2048, max_new_tokens=300):
        """
        Classify using Chain-of-Thought prompting for better reasoning
        
        Args:
            text: Text to classify
            max_length: Maximum context length for tokenization
            max_new_tokens: Maximum number of new tokens to generate
            
        Returns:
            Dictionary containing classification results
        """
        cot_prompt = f"""[INST] You are an expert in identifying regional biases in comments about Indian states and regions. 
Your task is to determine whether a comment contains regional bias or not.

Regional bias includes stereotypes, prejudices, or discriminatory statements about:
- Indian states or regions
- People from specific Indian states  
- Cultural, linguistic, economic, political, or infrastructural aspects of Indian regions

Please analyze the following comment step by step:

Comment: "{text}"

Step 1: First, identify if this comment mentions any Indian state, region, or people from specific regions.

Step 2: Check if the comment contains any of these elements:
- Stereotypical statements about people from a region
- Generalizations about a state or its people
- Discriminatory language targeting regional identity
- Prejudiced views about regional culture, language, or traditions
- Biased statements about economic or developmental status
- Political stereotypes associated with regions

Step 3: Determine if these elements, if present, constitute bias or are merely factual/neutral observations.

Step 4: Based on your analysis, classify this comment as:
- "regional_bias": If it contains prejudiced, stereotypical, or discriminatory content about Indian regions/states
- "non_regional_bias": If it's neutral, factual, or does not contain regional bias

Please provide your reasoning followed by your final classification.

Format your response as:
Reasoning: [Your step-by-step analysis]
Classification: [regional_bias/non_regional_bias] [/INST]
"""
        
        # Tokenize the prompt
        inputs = self.tokenizer(cot_prompt, return_tensors="pt", truncation=True, max_length=max_length)
        inputs = {key: value.to(self.device) for key, value in inputs.items()}
        
        # Generate response
        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=max_new_tokens,
                temperature=0.1,  # Low temperature for more deterministic outputs
                do_sample=False,  # Deterministic generation
                pad_token_id=self.tokenizer.pad_token_id,
                use_cache=True
            )
        
        # Decode the response
        response = self.tokenizer.decode(outputs[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)
        
        # Clear intermediate tensors
        del outputs, inputs
        torch.cuda.empty_cache()
        
        # Extract classification and reasoning
        try:
            lines = response.strip().split('\n')
            reasoning = ""
            classification = ""
            
            # Parse the response to extract reasoning and classification
            for line in lines:
                if line.startswith("Reasoning:"):
                    reasoning = line.replace("Reasoning:", "").strip()
                elif line.startswith("Classification:"):
                    classification = line.replace("Classification:", "").strip().lower()
            
            # Validate classification
            if "regional_bias" in classification:
                classification = "regional_bias"
            elif "non_regional_bias" in classification or "non-regional_bias" in classification:
                classification = "non_regional_bias"
            else:
                logger.warning(f"Invalid classification: {classification}")
                classification = "error"
            
            return {
                "classification": classification,
                "reasoning": reasoning,
                "full_response": response
            }
        except Exception as e:
            logger.error(f"Error parsing response: {e}")
            logger.debug(f"Full response: {response}")
            return {
                "classification": "error",
                "reasoning": "Error in parsing",
                "full_response": response
            }

def save_iteration_results(iteration_results, output_dir, iteration_num, timestamp):
    """
    Save results for a single iteration
    
    Args:
        iteration_results: List of result dictionaries for this iteration
        output_dir: Directory to save results
        iteration_num: Current iteration number
        timestamp: Timestamp for file naming
    
    Returns:
        Path to the saved predictions CSV file
    """
    # Create output directory if it doesn't exist
    os.makedirs(output_dir, exist_ok=True)
    
    # Save as JSON
    json_file = os.path.join(output_dir, f"mistral_iteration_{iteration_num}_results_{timestamp}.json")
    with open(json_file, 'w') as f:
        json.dump(iteration_results, f, indent=2)
    
    # Save as detailed CSV
    df_results = pd.DataFrame(iteration_results)
    csv_file = os.path.join(output_dir, f"mistral_iteration_{iteration_num}_detailed_{timestamp}.csv")
    df_results.to_csv(csv_file, index=False)
    
    # Save predictions only CSV
    predictions_data = []
    for result in iteration_results:
        predictions_data.append({
            'index': result['index'],
            'comment': result['original_comment'],
            'prediction': result['classification'],
            'prediction_binary': 1 if result['classification'] == 'regional_bias' else 0 if result['classification'] == 'non_regional_bias' else -1,
            'reasoning': result.get('reasoning', '')
        })
    
    df_predictions = pd.DataFrame(predictions_data)
    predictions_csv = os.path.join(output_dir, f"mistral_iteration_{iteration_num}_predictions_{timestamp}.csv")
    df_predictions.to_csv(predictions_csv, index=False)
    
    logger.info(f"Iteration {iteration_num} results saved:")
    logger.info(f"  - JSON: {json_file}")
    logger.info(f"  - Detailed CSV: {csv_file}")
    logger.info(f"  - Predictions CSV: {predictions_csv}")
    
    return predictions_csv

def generate_evaluation_report(all_iterations_predictions, ground_truth, all_iterations_results, output_dir):
    """
    Generate comprehensive evaluation report for multiple iterations
    
    Args:
        all_iterations_predictions: List of prediction lists for each iteration
        ground_truth: List of ground truth labels
        all_iterations_results: List of result lists for each iteration
        output_dir: Directory to save evaluation results
    """
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    # Calculate final predictions using majority voting
    final_predictions = []
    for i in range(len(ground_truth)):
        predictions_for_sample = [predictions[i] for predictions in all_iterations_predictions if i < len(predictions)]
        if predictions_for_sample:
            final_predictions.append(Counter(predictions_for_sample).most_common(1)[0][0])
    
    # Calculate metrics for final predictions
    report = classification_report(ground_truth, final_predictions, 
                                target_names=['Non-Regional Bias', 'Regional Bias'], 
                                output_dict=True)
    report_text = classification_report(ground_truth, final_predictions, 
                                      target_names=['Non-Regional Bias', 'Regional Bias'])
    conf_matrix = confusion_matrix(ground_truth, final_predictions)
    
    # Save classification report as text file
    report_file = os.path.join(output_dir, f"classification_report_mistral_{timestamp}.txt")
    with open(report_file, 'w') as f:
        f.write("=== Classification Report - Mistral (Multiple Iterations with Majority Voting) ===\n\n")
        f.write(f"Timestamp: {timestamp}\n")
        f.write(f"Total Samples: {len(final_predictions)}\n")
        f.write(f"Number of Iterations: {len(all_iterations_predictions)}\n\n")
        f.write(report_text)
        f.write("\n\n=== Confusion Matrix ===\n")
        f.write(str(conf_matrix))
        f.write("\n\nRows: Actual labels\n")
        f.write("Columns: Predicted labels\n")
        f.write("[0, 0] = True Negatives (Non-Regional Bias correctly classified)\n")
        f.write("[0, 1] = False Positives (Non-Regional Bias incorrectly classified as Regional)\n")
        f.write("[1, 0] = False Negatives (Regional Bias incorrectly classified as Non-Regional)\n")
        f.write("[1, 1] = True Positives (Regional Bias correctly classified)\n")
        
        # Add iteration-wise metrics
        f.write("\n\n=== Iteration-wise Performance ===\n")
        for i, predictions in enumerate(all_iterations_predictions):
            iter_report = classification_report(ground_truth[:len(predictions)], predictions, 
                                              target_names=['Non-Regional Bias', 'Regional Bias'], 
                                              output_dict=True)
            f.write(f"\nIteration {i+1} Accuracy: {iter_report['accuracy']:.4f}")
    
    # Create visualizations
    viz_dir = os.path.join(output_dir, "visualizations")
    os.makedirs(viz_dir, exist_ok=True)
    
    # 1. Final Confusion Matrix
    plt.figure(figsize=(8, 6))
    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', 
                xticklabels=['Non-Regional', 'Regional'],
                yticklabels=['Non-Regional', 'Regional'])
    plt.title('Confusion Matrix - Majority Voting from Multiple Iterations')
    plt.ylabel('True Label')
    plt.xlabel('Predicted Label')
    plt.tight_layout()
    confusion_matrix_file = os.path.join(viz_dir, f"confusion_matrix_mistral_{timestamp}.png")
    plt.savefig(confusion_matrix_file, dpi=300, bbox_inches='tight')
    plt.close()
    
    # 2. Iteration Agreement Visualization
    plt.figure(figsize=(10, 6))
    agreement_data = []
    for i in range(len(ground_truth)):
        predictions_for_sample = [predictions[i] for predictions in all_iterations_predictions if i < len(predictions)]
        if predictions_for_sample:
            agreement = len(set(predictions_for_sample)) == 1
            agreement_data.append(1 if agreement else 0)
    
    agreement_rate = sum(agreement_data) / len(agreement_data) if agreement_data else 0
    plt.bar(['Agreement', 'Disagreement'], 
            [sum(agreement_data), len(agreement_data) - sum(agreement_data)], 
            color=['green', 'red'])
    plt.title(f'Iteration Agreement Rate: {agreement_rate:.2%}')
    plt.ylabel('Number of Samples')
    agreement_file = os.path.join(viz_dir, f"iteration_agreement_mistral_{timestamp}.png")
    plt.savefig(agreement_file, dpi=300, bbox_inches='tight')
    plt.close()
    
    # 3. Class Distribution
    plt.figure(figsize=(10, 6))
    class_counts = [
        sum(label == 0 for label in ground_truth),
        sum(label == 1 for label in ground_truth)
    ]
    pred_counts = [
        sum(pred == 0 for pred in final_predictions),
        sum(pred == 1 for pred in final_predictions)
    ]
    
    x = np.arange(2)
    width = 0.35
    
    plt.bar(x - width/2, class_counts, width, label='Actual', color='skyblue')
    plt.bar(x + width/2, pred_counts, width, label='Predicted', color='lightcoral')
    
    plt.xlabel('Class')
    plt.ylabel('Count')
    plt.title('Class Distribution: Actual vs Predicted')
    plt.xticks(x, ['Non-Regional', 'Regional'])
    plt.legend()
    
    # Add count labels on bars
    for i, count in enumerate(class_counts):
        plt.text(i - width/2, count + 5, str(count), ha='center')
    for i, count in enumerate(pred_counts):
        plt.text(i + width/2, count + 5, str(count), ha='center')
    
    distribution_file = os.path.join(viz_dir, f"class_distribution_mistral_{timestamp}.png")
    plt.savefig(distribution_file, dpi=300, bbox_inches='tight')
    plt.close()
    
    # Save detailed evaluation report as JSON
    report_data = {
        'timestamp': timestamp,
        'total_samples': len(final_predictions),
        'accuracy': report['accuracy'],
        'metrics': {
            'non_regional_bias': report['Non-Regional Bias'],
            'regional_bias': report['Regional Bias'],
            'weighted_avg': report['weighted avg'],
            'macro_avg': report['macro avg']
        },
        'confusion_matrix': conf_matrix.tolist(),
        'predictions_distribution': {
            'non_regional_bias': int(np.sum(np.array(final_predictions) == 0)),
            'regional_bias': int(np.sum(np.array(final_predictions) == 1))
        },
        'iteration_agreement_rate': agreement_rate
    }
    
    json_report_file = os.path.join(output_dir, f"evaluation_report_mistral_{timestamp}.json")
    with open(json_report_file, 'w') as f:
        json.dump(report_data, f, indent=2)
    
    logger.info("\n=== Evaluation Report ===")
    logger.info(f"Total Samples: {len(final_predictions)}")
    logger.info(f"Accuracy: {report['accuracy']:.4f}")
    logger.info("\nClass-wise Metrics:")
    logger.info(f"Non-Regional Bias - Precision: {report['Non-Regional Bias']['precision']:.4f}, "
          f"Recall: {report['Non-Regional Bias']['recall']:.4f}, "
          f"F1: {report['Non-Regional Bias']['f1-score']:.4f}")
    logger.info(f"Regional Bias - Precision: {report['Regional Bias']['precision']:.4f}, "
          f"Recall: {report['Regional Bias']['recall']:.4f}, "
          f"F1: {report['Regional Bias']['f1-score']:.4f}")
    logger.info(f"\nFiles saved:")
    logger.info(f"- Classification Report: {report_file}")
    logger.info(f"- Confusion Matrix: {confusion_matrix_file}")
    logger.info(f"- Agreement Visualization: {agreement_file}")
    logger.info(f"- Class Distribution: {distribution_file}")
    logger.info(f"- Detailed JSON Report: {json_report_file}")

def main():
    """Main execution function"""
    # Parse command line arguments
    args = parse_args()
    
    # Create output directory
    os.makedirs(args.output_dir, exist_ok=True)
    
    # Extract model short name for naming files
    model_short_name = args.model_name.split('/')[-1] if '/' in args.model_name else args.model_name
    model_prefix = model_short_name.lower().split('-')[0]  # mistral, llama, etc.
    
    # Set up file logger
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    log_file = os.path.join(args.output_dir, f"{model_prefix}_classification_{timestamp}.log")
    file_handler = logging.FileHandler(log_file)
    file_handler.setFormatter(logging.Formatter('%(asctime)s - %(levelname)s - %(message)s'))
    logger.addHandler(file_handler)
    
    # Log execution parameters
    logger.info("=== Regional Bias Classification with Chain-of-Thought Reasoning ===")
    logger.info(f"Model: {args.model_name}")
    logger.info(f"Data path: {args.data_path}")
    logger.info(f"Output directory: {args.output_dir}")
    logger.info(f"Number of iterations: {args.num_iterations}")
    if hasattr(args, 'quantization'):
        logger.info(f"Quantization: {args.quantization}")
    if args.max_samples:
        logger.info(f"Max samples: {args.max_samples}")
    
    # Check GPU availability
    if not torch.cuda.is_available():
        logger.warning("No GPU available! This will be very slow.")
    else:
        logger.info(f"PyTorch version: {torch.__version__}")
        logger.info(f"CUDA version: {torch.version.cuda}")
        logger.info(f"Using GPU {args.gpu_id}: {torch.cuda.get_device_name(args.gpu_id)}")
    
    # Load data
    logger.info(f"Loading data from {args.data_path}")
    try:
        df = pd.read_csv(args.data_path)
        logger.info(f"Data shape: {df.shape}")
        logger.info(f"Columns: {df.columns.tolist()}")
        
        # Apply max_samples limit if specified
        if args.max_samples and args.max_samples < len(df):
            logger.info(f"Limiting to {args.max_samples} samples for processing")
            df = df.iloc[:args.max_samples]
    except Exception as e:
        logger.error(f"Error loading data: {e}")
        return
    
    try:
        # Login to HuggingFace if token provided
        if args.hf_token:
            login(token=args.hf_token)
            logger.info("Logged in to HuggingFace with provided token")
        elif os.environ.get("HF_TOKEN"):
            login(token=os.environ["HF_TOKEN"])
            logger.info("Logged in to HuggingFace using environment token")
        elif "llama" in args.model_name.lower():
            logger.warning("No HuggingFace token provided. LLaMA models require authentication.")
    except Exception as e:
        logger.warning(f"HuggingFace login failed: {e}")
        logger.warning("Proceeding without login - this may cause issues with access to gated models")
    
    # Initialize classifier with model-specific settings
    try:
        classifier_kwargs = {
            'model_name': args.model_name,
            'gpu_id': args.gpu_id,
            'cache_dir': args.cache_dir,
            'hf_token': args.hf_token
        }
        
        # Add quantization parameter if available
        if hasattr(args, 'quantization'):
            classifier_kwargs['quantization'] = args.quantization
            
        classifier = RegionalBiasClassifier(**classifier_kwargs)
    except Exception as e:
        logger.error(f"Error initializing classifier: {e}")
        return
    
    # Store results for all iterations
    all_iterations_results = []
    all_iterations_predictions = []
    ground_truth = []
    iteration_csv_files = []
    
    logger.info(f"\nStarting classification with Chain-of-Thought reasoning ({args.num_iterations} iterations)...")
    
    # Run multiple iterations
    for iteration in range(args.num_iterations):
        logger.info(f"\n=== Iteration {iteration + 1}/{args.num_iterations} ===")
        
        iteration_results = []
        iteration_predictions = []
        
        for idx, row in tqdm(df.iterrows(), total=len(df), desc=f"Iteration {iteration + 1}"):
            comment = str(row['Comment'])
            
            try:
                # Get classification with CoT
                result = classifier.classify_with_cot(
                    comment, 
                    max_length=args.max_length,
                    max_new_tokens=args.max_new_tokens
                )
                
                # Store results
                result['index'] = idx
                result['original_comment'] = comment
                result['iteration'] = iteration + 1
                iteration_results.append(result)
                
                # For evaluation
                if result['classification'] != 'error':
                    iteration_predictions.append(1 if result['classification'] == 'regional_bias' else 0)
                    
                    # Collect ground truth only in first iteration
                    if iteration == 0:
                        if 'Score' in df.columns:  # Assuming Score > 0 means regional bias
                            ground_truth.append(1 if float(row['Score']) > 0 else 0)
                        elif 'Level-1' in df.columns:  # Alternative ground truth column
                            ground_truth.append(1 if float(row['Level-1']) > 0 else 0)
                
            except Exception as e:
                logger.error(f"Error processing comment {idx} in iteration {iteration + 1}: {e}")
                iteration_results.append({
                    "index": idx,
                    "original_comment": comment,
                    "classification": "error",
                    "reasoning": f"Error: {str(e)}",
                    "full_response": "",
                    "iteration": iteration + 1
                })
            
            # Clear GPU cache periodically
            if (idx + 1) % 50 == 0:
                torch.cuda.empty_cache()
                gc.collect()
        
        # Store iteration results
        all_iterations_results.append(iteration_results)
        all_iterations_predictions.append(iteration_predictions)
        
        # Save iteration results (JSON, detailed CSV, and predictions CSV)
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        predictions_csv = save_iteration_results(
            iteration_results, args.output_dir, 
            iteration + 1, timestamp, model_prefix
        )
        iteration_csv_files.append(predictions_csv)
        
        logger.info(f"Iteration {iteration + 1} completed.")
    
    # Combine results and calculate final predictions
    final_results = []
    for idx in range(len(df)):
        comment_results = []
        for iteration_results in all_iterations_results:
            for result in iteration_results:
                if result['index'] == idx:
                    comment_results.append(result)
        
        # Get classifications from all iterations
        classifications = [r['classification'] for r in comment_results if r['classification'] != 'error']
        
        # Use majority voting for final classification
        if classifications:
            final_classification = Counter(classifications).most_common(1)[0][0]
        else:
            final_classification = 'error'
        
        final_results.append({
            'index': idx,
            'original_comment': df.iloc[idx]['Comment'],
            'final_classification': final_classification,
            'iteration_classifications': classifications,
            'all_iteration_results': comment_results
        })
    
    # Save final combined results
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    final_results_file = os.path.join(args.output_dir, f"{model_prefix}_final_results_{timestamp}.json")
    with open(final_results_file, 'w') as f:
        json.dump(final_results, f, indent=2)
    
    # Save final combined CSV
    final_df_data = []
    for result in final_results:
        final_df_data.append({
            'index': result['index'],
            'comment': result['original_comment'],
            'final_classification': result['final_classification'],
            'final_classification_binary': 1 if result['final_classification'] == 'regional_bias' else 0 if result['final_classification'] == 'non_regional_bias' else -1,
            'iteration_1': result['iteration_classifications'][0] if len(result['iteration_classifications']) > 0 else 'N/A',
            'iteration_2': result['iteration_classifications'][1] if len(result['iteration_classifications']) > 1 else 'N/A',
            'iteration_3': result['iteration_classifications'][2] if len(result['iteration_classifications']) > 2 else 'N/A',
            'num_iterations_completed': len(result['iteration_classifications']),
            'all_agree': len(set(result['iteration_classifications'])) == 1 if result['iteration_classifications'] else False
        })
    
    final_df = pd.DataFrame(final_df_data)
    final_csv_file = os.path.join(args.output_dir, f"{model_prefix}_final_combined_{timestamp}.csv")
    final_df.to_csv(final_csv_file, index=False)
    
    logger.info(f"\nFinal combined results saved to:")
    logger.info(f"- JSON: {final_results_file}")
    logger.info(f"- CSV: {final_csv_file}")
    
    # Generate evaluation report if ground truth is available
    if ground_truth:
        generate_evaluation_report(
            all_iterations_predictions, ground_truth, all_iterations_results, 
            args.output_dir, args.model_name, model_prefix
        )
    else:
        logger.warning("No ground truth available for evaluation")
    
    logger.info("\nAll iteration CSV files:")
    for i, csv_file in enumerate(iteration_csv_files):
        logger.info(f"Iteration {i+1}: {csv_file}")

if __name__ == "__main__":
    main()
